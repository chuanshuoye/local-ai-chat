 # AI 模型设置指南

## 模型概述

在我们的聊天应用中，您可以选择不同的AI模型来满足各种对话需求。每个模型都有其特点和适用场景。

## 可用模型

我们的应用支持以下AI模型：

| 模型 | 特点 | 适用场景 |
| --- | --- | --- |
| llama 3.2 | 高性能通用大语言模型 | 日常对话、通用问答 |
| deepseek-coder-v2 | deepseek高效开源语言模型 | 代码编写、技术解决方案 |
| deepseek-r1:8b | 高效开源语言模型 | 一般对话、快速响应场景 |
| deepseek-r1:14b | Deepseek开源模型 | 更复杂的任务和推理 |

## 模式设置

### 选择模型

在聊天界面的左侧边栏底部，点击设置图标打开设置面板，您可以：

1. 从下拉菜单中选择您想使用的AI模型
2. 查看当前选择模型的简要说明

### 流式响应

您可以选择是否启用流式响应：

- **启用流式响应**：AI生成的内容将逐步显示，就像实时打字一样
- **关闭流式响应**：AI等待全部内容生成完成后再一次性显示

流式响应能提供更自然的对话体验，但在某些场景下（如代码生成）可能需要关闭以获得更好的格式。

## 自定义助手

除了选择基础模型外，您还可以使用或创建自定义助手：

1. 在左侧边栏的"助手市场"部分浏览预设助手
2. 点击"添加自定义助手"创建您自己的专业助手
3. 设置助手名称、描述、图标和角色提示词

## 模型选择建议

- **llama 3.2**：适合大多数日常对话和一般性问题
- **deepseek-coder-v2**：编程相关问题的首选，适合代码生成和技术解答
- **deepseek-r1:8b**：资源占用较小，适合简单快速的问答
- **deepseek-r1:14b**：在需要更深入理解和推理的场景中使用

## 最佳实践

- 根据问题的性质选择适合的模型
- 为特定领域的问题创建专门的自定义助手
- 处理代码相关问题时，优先选择 deepseek-coder-v2
- 通过调整提示词来获得更精准的回答

---

通过合理选择模型和设置，您可以获得更高效、更精准的AI对话体验。